{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5rfduRD1CSF"
      },
      "source": [
        "# Create your own private Copilot\n",
        "\n",
        "**In this guide I show you how to fine-tune Code Llama to become a private Copilot. For coding tasks, you can generally get much better performance out of Code Llama than Llama 2, especially when you specialise the model on a particular task:**\n",
        "\n",
        "- A Lora approach, quantizing the base model to int 8, freezing its weights and only training an adapter\n",
        "- Much of the code is refactored from [alpaca-lora](https://github.com/tloen/alpaca-lora).\n",
        "\n",
        "Avoid running this on V100 GPUs as [BF16 is not supported on V100](https://github.com/facebookresearch/llama-recipes/issues/284) and will otherwise throw out errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6lxQKE5MW310"
      },
      "outputs": [],
      "source": [
        "# Install python dependencies\n",
        "!pip install tqdm nbformat\n",
        "!pip install git+https://github.com/huggingface/transformers.git@main bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/peft.git@main\n",
        "!pip install datasets\n",
        "import locale # colab workaround\n",
        "locale.getpreferredencoding = lambda x=False:\"UTF-8\" # colab workaround\n",
        "!pip install wandb\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and install git-xet\n",
        "!curl -fsSLO https://github.com/xetdata/xet-tools/releases/latest/download/xet-linux-x86_64.tar.gz\n",
        "!tar -xvf xet-linux-x86_64.tar.gz && rm xet-linux-x86_64.tar.gz\n",
        "!mv git-xet /usr/local/bin\n",
        "!git xet install"
      ],
      "metadata": {
        "id": "hJ-nvsLiXqU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A5O9EwlW311"
      },
      "outputs": [],
      "source": [
        "# Set up authorization\n",
        "from IPython.display import clear_output\n",
        "user = input(\"GitHub user name?\")\n",
        "%env GH_USER=$user\n",
        "email = input(\"GitHub user email?\")\n",
        "%env GH_USER_EMAIL=$email\n",
        "token = input(\"GitHub token?\")\n",
        "%env GH_TOKEN=$token\n",
        "repo = input(\"GitHub model repo?\")\n",
        "%env MODEL_REPO=$repo\n",
        "%env XET_LOG_PATH=log.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name $GH_USER\n",
        "!git config --global user.email $GH_USER_EMAIL"
      ],
      "metadata": {
        "id": "Q-rjZRSXX4X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clones the model repo\n",
        "!git xet clone --lazy https://$GH_USER:$GH_TOKEN@github.com/$GH_USER/$MODEL_REPO.git -- --branch colab\n",
        "!cd LLM_fine_tuning && git xet materialize CodeLlama-7b-hf"
      ],
      "metadata": {
        "id": "Vs7NWkwmXKXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KXXSZVwKW311"
      },
      "outputs": [],
      "source": [
        "# Get training dataset\n",
        "from LLM_fine_tuning.scripts.prepare_dataset import create_dataset_from_git_repo\n",
        "username='xetdata'\n",
        "repository='xet-core'\n",
        "parquet_file = create_dataset_from_git_repo(username,repository)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3tvZaQ7fW312"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(parquet_file)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mu9JczX1CSH"
      },
      "source": [
        "### Loading libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTeYW8z51CSH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    prepare_model_for_int8_training,\n",
        "    set_peft_model_state_dict,\n",
        ")\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zH9-hM1CSH"
      },
      "source": [
        "(If you have import errors, try restarting your Jupyter kernel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M9KyT0S1CSH"
      },
      "source": [
        "### Load dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w44O1EK-1CSH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, load_dataset\n",
        "dataset = Dataset.from_pandas(df, split=\"train\")\n",
        "train_dataset = dataset.train_test_split(test_size=0.1)[\"train\"]\n",
        "eval_dataset = dataset.train_test_split(test_size=0.1)[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFbeaZzf1CSJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(train_dataset[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Xb42Xar6W313"
      },
      "outputs": [],
      "source": [
        "print(eval_dataset[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ig7NvWN1CSJ"
      },
      "source": [
        "### Load model\n",
        "I load code llama from huggingface in int8. Standard for Lora:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMnU93bY1CSJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "base_model = \"./LLM_fine_tuning/CodeLlama-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3OF-wtj1CSJ"
      },
      "source": [
        "torch_dtype=torch.float16 means computations are performed using a float16 representation, even though the values themselves are 8 bit ints.\n",
        "\n",
        "If you get error \"ValueError: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.\" Make sure you have transformers version is 4.33.0.dev0 and accelerate is >=0.20.3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VXqJJe1CSJ"
      },
      "source": [
        "### 3. Check base model\n",
        "A very good common practice is to check whether a model can already do the task at hand. Fine-tuning is something you want to try to avoid at all cost:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiyAff1a1CSJ"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"You are a powerful code generation model. Your job is to complete the below Rust function.\n",
        "/// Walk the repo working directory starting from search_root.\n",
        "/// Return a list of file paths under the search_root, the\n",
        "/// file paths are relative to the working dir root.\n",
        "/// Note that symlinks are ignored because they are difficult to\n",
        "/// deal with: git deals with the symlink file itself without\n",
        "/// following the link.\n",
        "pub fn walk_working_dir(\n",
        "    work_root: impl AsRef<Path>,\n",
        "    search_root: impl AsRef<Path>,\n",
        "    recursive: bool,\n",
        ") -> anyhow::Result<Vec<PathBuf>> {\n",
        "  <FILL_ME>\n",
        "\n",
        "  Ok(result)\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544rmsdU1CSJ"
      },
      "source": [
        "I get the output:\n",
        "```\n",
        "llet work_root = work_root.as_ref();\n",
        "  let search_root = search_root.as_ref();\n",
        "  let mut result = Vec::new();\n",
        "  let mut stack = Vec::new();\n",
        "  stack.push(search_root.to_path_buf());\n",
        "  while let Some(path) = stack.pop() {\n",
        "    if path.starts_with(work_root) {\n",
        "      for entry in path.read\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puuOXL2R1CSJ"
      },
      "source": [
        "### 4. Tokenization\n",
        "Setup some tokenization settings like left padding because it makes [training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1P0mphA1CSJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_eos_token = True\n",
        "tokenizer.pad_token_id = 0\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo66hMo1CSJ"
      },
      "source": [
        "Setup the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning](https://neptune.ai/blog/self-supervised-learning) is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjSerml71CSJ"
      },
      "outputs": [],
      "source": [
        "def tokenize(prompt):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # \"self-supervised learning\" means the labels are also the inputs:\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvO7A-ZF1CSK"
      },
      "outputs": [],
      "source": [
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt =f\"\"\"\n",
        "    ### Repository:\n",
        "    {data_point[\"repo_id\"]}\n",
        "\n",
        "    ### File Path:\n",
        "    {data_point[\"file_path\"]}\n",
        "\n",
        "    ### Source Code:\n",
        "    {data_point[\"content\"]}\n",
        "    \"\"\"\n",
        "    return tokenize(full_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLvwo-ax1CSK"
      },
      "source": [
        "Reformat to prompt and tokenize each sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SA2BqSzW1CSK"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ByhitV1CSK"
      },
      "source": [
        "### 5. Setup Lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q_26pz71CSK"
      },
      "outputs": [],
      "source": [
        "model.train() # put model back into training mode\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "    \"q_proj\",\n",
        "    \"k_proj\",\n",
        "    \"v_proj\",\n",
        "    \"o_proj\",\n",
        "],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW1wtvbP1CSK"
      },
      "source": [
        "Optional stuff to setup Weights and Biases to view training graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC1_qWbS1CSK"
      },
      "outputs": [],
      "source": [
        "wandb_project = \"\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iv4txxu1CSK"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.device_count() > 1:\n",
        "    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYSsnciQ1CSK"
      },
      "source": [
        "### 6. Training arguments\n",
        "If you run out of GPU memory, change per_device_train_batch_size. The gradient_accumulation_steps variable should ensure this doesn't affect batch dynamics during the training run. All the other variables are standard stuff that I wouldn't recommend messing with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvEi1kP21CSK"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "per_device_train_batch_size = 8\n",
        "gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
        "output_dir = \"code-llama\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=100,\n",
        "        max_steps=400,\n",
        "        learning_rate=3e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_torch\",\n",
        "        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=20,\n",
        "        save_steps=20,\n",
        "        output_dir=output_dir,\n",
        "        # save_total_limit=3,\n",
        "        load_best_model_at_end=False,\n",
        "        # ddp_find_unused_parameters=False if ddp else None,\n",
        "        group_by_length=True, # group sequences of roughly the same length together to speed up training\n",
        "        report_to=\"none\", # if use_wandb else \"none\",\n",
        "        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(\n",
        "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aJp6Jxl1CSK"
      },
      "source": [
        "Then we do some pytorch-related optimisation (which just make training faster but don't affect accuracy):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ycCHZZl1CSK"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False\n",
        "\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    print(\"compiling the model\")\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF5oWKxK1CSL"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "model.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1dRQLMT1CSU"
      },
      "source": [
        "### Load the final checkpoint\n",
        "Now for the moment of truth! Has our work paid off...?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRdVgDTg1CSU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "base_model = \"./LLM_fine_tuning/CodeLlama-7b-hf\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76UlHzhy1CSU"
      },
      "source": [
        "To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained. ```output_dir``` should be something containing an adapter_config.json and adapter_model.bin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQrCR0os1CSU"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "model = PeftModel.from_pretrained(model, output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roqy_WRi1CSU"
      },
      "source": [
        "Try the same prompt as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrCxouNp1CSU"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"You are a powerful code generation model. Your job is to complete the below Rust function.\n",
        "/// Walk the repo working directory starting from search_root.\n",
        "/// Return a list of file paths under the search_root, the\n",
        "/// file paths are relative to the working dir root.\n",
        "/// Note that symlinks are ignored because they are difficult to\n",
        "/// deal with: git deals with the symlink file itself without\n",
        "/// following the link.\n",
        "pub fn walk_working_dir(\n",
        "    work_root: impl AsRef<Path>,\n",
        "    search_root: impl AsRef<Path>,\n",
        "    recursive: bool,\n",
        ") -> anyhow::Result<Vec<PathBuf>> {\n",
        "  <FILL_ME>\n",
        "\n",
        "  Ok(result)\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6IcTOCq1CSU"
      },
      "source": [
        "And the model outputs:\n",
        "```\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally merge the adapter and save the model\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(base_model)\n",
        "!cd LLM_fine_tuning && git add . && git commit -m \"Update fine tuned model\" && git push"
      ],
      "metadata": {
        "id": "yHEgbSFyMJTN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}